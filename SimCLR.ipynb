{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "vqICKYNhyY-2",
        "Qzox2VOWyyBa",
        "pU0ix1moy_cf"
      ],
      "mount_file_id": "1ZS9xSRmgiIN3_dK3ambNcQdw_Vrf-ZbA",
      "authorship_tag": "ABX9TyPMbyf+kekUmShmc+whPNKt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abibalimi/self-supervised/blob/main/SimCLR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reproducing SimCLR"
      ],
      "metadata": {
        "id": "uEqSMNfzxcMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet18\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "aTU98IsrxXtR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "QP9uB1--Q6mN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Steps:\n"
      ],
      "metadata": {
        "id": "vqICKYNhyY-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Data Augmentation\n",
        "2.   Encoder Network\n",
        "3.   Projection Head\n",
        "4.   Contrastive Loss"
      ],
      "metadata": {
        "id": "5wqZX9_Zx2i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameters"
      ],
      "metadata": {
        "id": "Xqg00CZPxs6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024\n",
        "BASE_LR = 1e-3 #3 * BATCH_SIZE / 256  # Learning rate = 1.2\n",
        "WEIGHT_DECAY = 1e-6\n",
        "WARM_UP_RATE = 0.4\n",
        "TEMPERATURE = 0.5\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "wXv_vM2KxbAu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#          ***         Data Augmentation         ***         #"
      ],
      "metadata": {
        "id": "9Ffa-5QpygGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Albumentations augmentations\n",
        "augmentation = A.Compose([\n",
        "    # Inception-style cropping: random crop, flip, and resize to 32x32\n",
        "    A.RandomResizedCrop((32, 32), scale=(0.08, 1.0)),\n",
        "    A.HorizontalFlip(),\n",
        "    A.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])"
      ],
      "metadata": {
        "id": "iWKI36ZDxruf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset to apply Albumentations\n",
        "class AugmentedDataset(Dataset):\n",
        "    def __init__(self, dataset, augment):\n",
        "        self.dataset = dataset\n",
        "        self.augmentation = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[idx]\n",
        "        image = np.array(image)  # Convert PIL Image to numpy array\n",
        "        x1 = self.augmentation(image=image)['image']\n",
        "        x2 = self.augmentation(image=image)['image']\n",
        "        return x1, x2, label\n"
      ],
      "metadata": {
        "id": "_Ho4VUoQybbU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#          ***         Encoder Network (ResNet-18)         ***         #"
      ],
      "metadata": {
        "id": "Qzox2VOWyyBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.backbone = resnet18()   # Random initialization\n",
        "        self.backbone.fc = nn.Identity()  # Remove the final classification layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)"
      ],
      "metadata": {
        "id": "3tGglp3zyvUz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#          ***         Projection Head         ***         #"
      ],
      "metadata": {
        "id": "pU0ix1moy_cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, input_dim=512, hidden_dim=256, output_dim=128):\n",
        "        super(ProjectionHead, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KbiBDQDGyvrL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#          ***         SimCLR Model         ***         #"
      ],
      "metadata": {
        "id": "E4ivOxt7zKLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, encoder, projection_head):\n",
        "        super(SimCLR, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.projection_head = projection_head\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Encode the two augmented views\n",
        "        h1 = self.encoder(x1)\n",
        "        h2 = self.encoder(x2)\n",
        "\n",
        "        # Project to the lower-dimensional space\n",
        "        z1 = self.projection_head(h1)\n",
        "        z2 = self.projection_head(h2)\n",
        "\n",
        "        return z1, z2"
      ],
      "metadata": {
        "id": "dPzqhhz5zHCf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#          ***         Contrastive Loss (NT-Xent)         ***         #"
      ],
      "metadata": {
        "id": "MAACWewqzR6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_loss(z1, z2, temperature=0.1):\n",
        "    BATCH_SIZE = z1.size(0)\n",
        "    z = torch.cat([z1, z2], dim=0)  # Concatenate both views\n",
        "    z = nn.functional.normalize(z, dim=1)  # Normalize feature vectors\n",
        "\n",
        "    # Compute similarity matrix\n",
        "    sim_matrix = torch.matmul(z, z.T) / temperature\n",
        "\n",
        "    # Create labels for positive pairs\n",
        "    labels = torch.arange(BATCH_SIZE, device=z.device)\n",
        "    labels = torch.cat([labels + BATCH_SIZE, labels])  # Positive pairs are diagonal elements\n",
        "\n",
        "    # Compute cross-entropy loss\n",
        "    loss = nn.functional.cross_entropy(sim_matrix, labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "SVy7bDSQzSv8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "YutwuuB0zb8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load CIFAR-10 dataset"
      ],
      "metadata": {
        "id": "qaaePlkvzr4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize the model"
      ],
      "metadata": {
        "id": "Ow6P0dG9zjtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Learning rate scheduler"
      ],
      "metadata": {
        "id": "X-_EBPpOzvj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_scheduler(optimizer, epochs=EPOCHS, warm_up_rate=WARM_UP_RATE):\n",
        "    \"\"\"Schedules the learning rate\"\"\"\n",
        "    warmup_epochs = epochs * warm_up_rate  # 10%\n",
        "    total_epochs = epochs # 100\n",
        "    warmup_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=warmup_epochs)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epochs - warmup_epochs)\n",
        "    return scheduler, warmup_scheduler, warmup_epochs"
      ],
      "metadata": {
        "id": "EZFEQDcZzwVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save checkpoints"
      ],
      "metadata": {
        "id": "2BkpvMNpz59V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(epoch, model, optimizer, scheduler, loss, checkpoint_dir):\n",
        "    \"\"\" Function to save checkpoint\"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': loss\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_dir / f\"simclr_checkpoint_epoch_{epoch+1}.pth\")\n",
        "    print(f\"✅ Checkpoint saved at epoch {epoch}\")"
      ],
      "metadata": {
        "id": "ZCKIynGbz40u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -Uq"
      ],
      "metadata": {
        "id": "gSFkSxSd6ljC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "RXXOr_pN6o91"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "T2wKS9SV6rJq",
        "outputId": "7c459f60-f4b9-4d63-8ef6-d63f0646fc83"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molush\u001b[0m (\u001b[33molush-ai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a sweep"
      ],
      "metadata": {
        "id": "tYKSaW7jDuG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "    }"
      ],
      "metadata": {
        "id": "d8GzAO0n7AHm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = {\n",
        "    'name': 'loss',\n",
        "    'goal': 'minimize'\n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric"
      ],
      "metadata": {
        "id": "ObQzuQjQ7BVc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_dict = {\n",
        "    'optimizer': {\n",
        "        'values': ['adam', 'sgd']\n",
        "        }\n",
        "    }\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict"
      ],
      "metadata": {
        "id": "KD8s0_Kz7JJe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_dict.update({\n",
        "    'learning_rate': {\n",
        "        # a flat distribution between 1e-3 and 0.1\n",
        "        'distribution': 'uniform',\n",
        "        'min': 1e-3,\n",
        "        'max': 1.\n",
        "      },\n",
        "    'batch_size': {\n",
        "        # integers between 128 and 1024\n",
        "        # with evenly-distributed logarithms\n",
        "        'distribution': 'q_log_uniform_values',\n",
        "        'q': 8,\n",
        "        'min': 128,\n",
        "        'max': 1024,\n",
        "      }\n",
        "    })"
      ],
      "metadata": {
        "id": "aa3rkQEE7XdW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_dict.update({\n",
        "    'epochs': {\n",
        "        'value': 10}\n",
        "    })"
      ],
      "metadata": {
        "id": "agpkbcedC-MN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config['backbone'] = 'ResNet18'"
      ],
      "metadata": {
        "id": "vq341--PNrw-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "pprint.pprint(sweep_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK7RYTPdDK-Z",
        "outputId": "a992f3c1-721e-4826-e47c-b470796bcc8a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'backbone': 'ResNet18',\n",
            " 'method': 'random',\n",
            " 'metric': {'goal': 'minimize', 'name': 'loss'},\n",
            " 'parameters': {'batch_size': {'distribution': 'q_log_uniform_values',\n",
            "                               'max': 1024,\n",
            "                               'min': 128,\n",
            "                               'q': 8},\n",
            "                'epochs': {'value': 10},\n",
            "                'learning_rate': {'distribution': 'uniform',\n",
            "                                  'max': 1.0,\n",
            "                                  'min': 0.001},\n",
            "                'optimizer': {'values': ['adam', 'sgd']}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the Sweep"
      ],
      "metadata": {
        "id": "uJTD6HJMD7Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"simCLR-sweeps\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUr7IyMXER2Y",
        "outputId": "a242546d-7833-4c9f-faff-605088c20d5d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: u16zfjto\n",
            "Sweep URL: https://wandb.ai/olush-ai/simCLR-sweeps/sweeps/u16zfjto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define SimCLR code\n"
      ],
      "metadata": {
        "id": "H5h1InsvEz7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config=None):\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config):\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "        config = wandb.config\n",
        "\n",
        "        loader = build_dataset(config.batch_size)\n",
        "        network = build_network()\n",
        "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
        "\n",
        "        # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "        wandb.watch(network, log=\"all\", log_freq=10)\n",
        "\n",
        "\n",
        "        #for epoch in range(config.epochs):\n",
        "        for epoch in tqdm(range(config.epochs)):\n",
        "            avg_loss = train_epoch(network, loader, optimizer)\n",
        "            wandb.log({\"loss\": avg_loss, \"epoch\": epoch})\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Losses :: Train = {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "fi2Z5VwSDks-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(batch_size):\n",
        "    \"\"\"Loads split datasets\"\"\"\n",
        "    # download CIFAR10 training dataset\n",
        "    dataset = CIFAR10(root='./data', train=True, download=True)\n",
        "    augmented_dataset = AugmentedDataset(dataset, augmentation)\n",
        "    loader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def build_network():\n",
        "    \"\"\"Initializes the model/network\"\"\"\n",
        "    encoder = Encoder().to(device)\n",
        "    projection_head = ProjectionHead().to(device)\n",
        "    model = SimCLR(encoder, projection_head).to(device)\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "def build_optimizer(network, optimizer, learning_rate):\n",
        "    \"\"\"Initializes the optimizer\"\"\"\n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(network.parameters(),\n",
        "                              lr=learning_rate, momentum=0.9,\n",
        "                              weight_decay=WEIGHT_DECAY)\n",
        "    elif optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(network.parameters(),\n",
        "                               lr=learning_rate,\n",
        "                               weight_decay=WEIGHT_DECAY)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, optimizer):\n",
        "    cumu_loss = 0\n",
        "    #for _, (data, target) in enumerate(loader):\n",
        "    for _, (x1, x2, _) in enumerate(loader):\n",
        "        # Zero the gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move data to device and make predictions for this batch (Forward pass)\n",
        "        x1, x2 = x1.to(device), x2.to(device)\n",
        "\n",
        "        # ➡ Forward pass : Compute contrastive loss\n",
        "        z1, z2 = model(x1, x2)\n",
        "        loss = contrastive_loss(z1, z2, TEMPERATURE)\n",
        "\n",
        "        # ⬅ Backward pass + weight update\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        wandb.log({\"batch loss\": loss.item()})\n",
        "\n",
        "    return cumu_loss / len(loader)\n",
        "\n",
        "\n",
        "def val_epoch(model, loader, optimizer):\n",
        "    cumu_loss = 0\n",
        "    # Set the model to evaluation/validation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x1, x2, _) in enumerate(loader):\n",
        "            # Move data to device and predict\n",
        "            x1, x2 = x1.to(device), x2.to(device)\n",
        "            z1, z2 = model(x1, x2)\n",
        "\n",
        "            # Compute contrastive loss\n",
        "            val_loss = contrastive_loss(z1, z2, TEMPERATURE)\n",
        "            per_epoch_val_loss += val_loss.item()\n",
        "\n",
        "    wandb.log({\"val batch loss\": val_loss.item()})\n",
        "\n",
        "    return cumu_loss / len(loader)"
      ],
      "metadata": {
        "id": "1JquzJ045KYu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activate sweep agents"
      ],
      "metadata": {
        "id": "CekC3mddCuEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train, count=10)"
      ],
      "metadata": {
        "id": "9nHmY0gNCwOa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}